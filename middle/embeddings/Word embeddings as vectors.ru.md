**Привет, друзья! Сегодня объясню, как можно превратить слова в числа так, чтобы компьютер с этим мог работать. Не переживайте, всё будет максимально понятно, даже если вы только начинаете программировать!**

⚠️ Файл для практики - [Word embeddings as vectors.py](Word%20embeddings%20as%20vectors.py)

### Почему вообще нужно превращать слова в числа?

Когда мы пишем или разговариваем, мы используем слова: например, фраза “Кошка любит молоко”. Всё просто и понятно для нас.  
Но для компьютера — особенно для искусственного интеллекта или нейросети — слова это непонятные символы. Компьютер умеет складывать и вычитать числа, но слова — это не числа. Мы можем начать выполнять мощные математические операции над словами, чтобы обнаружить их сходства.

**Задача**: Научиться представлять слова в виде чисел, чтобы компьютер мог с ними что-то делать.

---

### Первый глупый способ — давать каждому слову случайное число

Допустим, есть два предложения:

- “Программирование классное!”
- “Программирование крутое!”

Мы можем просто каждому слову дать случайное число, например:
- “Программирование” → 1.1
- “классное” → 7.3
- “крутое” → -0.5
Но тут возникает проблема!  
Хотя “классное” и “крутое” — это почти одно и то же по смыслу, их числа совсем не похожи. Для компьютера это как будто вообще разные вещи, и он не сможет сам догадаться, что это похоже.

---

### Хочется, чтобы похожие слова имели похожие числа!

**Почему это важно?**  
Когда ИИ учится на одном примере, он бы мог “перенести опыт” на похожее слово. Например, если он понял, что “крутое” — хорошо, ему легко будет догадаться, что “классное” — тоже, потому что их числа близки.

---

### Как это можно сделать? (Введение в эмбеддинги слов)

**Вот тут появляется магия — эмбеддинги слов (word embeddings)**

- *Эмбеддинг* — это просто набор чисел для каждого слова, которые как раз и “кодируют” смысл слова, чтобы похожие слова были рядом по этим числам.
- Например:
    - “крутое” → (0.9, 1.1)
    - “классное” → (0.95, 1.05)
Видите? У них похожие наборы чисел!

---

### Как сделать эти эмбеддинги?

**Вместо того чтобы случайно назначать числа, мы можем заставить нейросеть подобрать их сама.**

1. Берём тексты (чаще всего — большие, прямо всю Википедию)
2. Учим нейросетку “угадывать” слова по соседям.
    - Например, из фразы “Программирование классное”, нейросеть пытается, зная “Программирование”, угадать, что следующее слово “классное”.
3. В процессе того, как сеть учится, она “двигает” наборы чисел для каждого слова так, чтобы получилось угадывать лучше.
4. В результате, слова, которые встречаются в похожих контекстах (“крутое”, “классное”) получают похожие числа (эмбеддинги).
**Пример лично для тебя:** Если “кошка любит молоко” и “собака любит еду”, то слова “кошка” и “собака” со временем станут близки друг к другу по наборам чисел, ведь и у них общие контексты (любят что-то есть).

---

### Чуть глубже: Архитектура нейросети для эмбеддингов

(Очень просто)

- На входе — все слова (каждое это нолики и единичка, чтобы сказать “это слово сейчас на входе”)
- Для каждого слова — набор чисел (эмбеддинг)
- Сеть учится угадывать соседние слова
- Веса, которые сеть назначила каждому слову — это и есть те самые искомые “эмбеддинги”.

---

### Крутая штука word2vec

Одно из самых известных решений — это word2vec. Это способ научить компьютер понимать, какие слова похожи по смыслу, с помощью математики. Есть 2 способа обучения нейросети:

1. **Continuous bag-of-words (CBOW):**  
   Тут сеть пытается по окружающим словам предсказать слово в середине.  
   Пример: по “кот ___ молоко” сеть учится догадаться, что в середине должно быть “любит”.

2. **Skip-gram**  
   Тут наоборот — сеть учится по слову в середине предугадывать, какие слова могут быть вокруг.  
   Пример: если видим “любит”, сеть старается догадаться, что вокруг надписи могут стоять “кот” и “молоко”.

Подробнее вы можете почитать тут - https://medium.com/@zafaralibagh6/a-simple-word2vec-tutorial-61e64e38a6a1

---

### А если слов очень много?

В реальности слов тысячи, иногда миллионы!  
У каждой нейросети для каждого слова должны быть свои числа… Получается огромное количество параметров — обучать сложно и долго.

Чтобы это ускорить, word2vec использует **Negative Sampling**:

- Вместо того, чтобы “сравнивать” каждое слово со всеми остальными, сеть “сравнивает” только с небольшой случайной подборкой “неправильных” слов.
- Это как если бы вам нужно было из огромного списка найти друга, но вы смотрите только на пару случайных людей для сравнения.
#### Пример:

Предложение:  
**"Кошка любит молоко"**

Центральное слово:  
**"любит"**

Окно контекста:  
**1 слово слева и справа**

#### Положительные пары (positive samples):

- ("любит", "Кошка")  
- ("любит", "молоко")

---

#### Проблема:
Чтобы хорошо обучить модель, она должна понимать не только, какие пары **правильные**, но и какие — **неправильные**.

Но если в словаре **миллионы** слов, сравнивать каждую пару со всеми остальными — **очень долго и дорого**.

---

#### Решение: Negative Sampling

Мы **берём несколько случайных слов**, которые НЕ были рядом с "любит", и обучаем модель на том, что эти пары — **неправильные**.

Например:

- ("любит", "автобус")  
- ("любит", "телефон")  
- ("любит", "луна")

---

##### Что видит модель:

| Центр   | Контекст | Метка |
|---------|----------|-------|
| любит   | кошка    | 1     |
| любит   | молоко   | 1     |
| любит   | автобус  | 0     |
| любит   | телефон  | 0     |
| любит   | луна     | 0     |

---

##### Что делает модель:
- Сближает положительные пары (где метка 1)
- Раздвигает отрицательные пары (где метка 0)

Это делает обучение **намного быстрее**, но при этом результат остаётся **качественным**.

---

### Итог:

- Слова «переводятся» в наборы чисел, чтобы компьютер мог с ними работать.
- Похожие слова получают похожие числа — и это помогает ИИ понимать текст лучше.
- Для этого используется нейросеть, которая учится на огромном количестве текстов.
- Есть разные умные методы (word2vec, Negative Sampling), которые позволяют делать это быстрее и лучше.
